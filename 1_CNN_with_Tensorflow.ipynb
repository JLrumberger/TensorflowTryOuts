{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 CNN with Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_-47X6XN96Ev",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JLrumberger/TensorflowTryOuts/blob/master/1%20CNN%20with%20Tensorflow.ipynb)\n",
        "## CNN with Tensorflow\n",
        "We first load the dataset and construct the placeholders for the input data and targets."
      ]
    },
    {
      "metadata": {
        "id": "zAmS-1M796E1",
        "colab_type": "code",
        "outputId": "60ab2795-1045-41cd-cce3-bf0ae2661ef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "#mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data(path='mnist.npz')\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test, depth=10)\n",
        "\n",
        "\n",
        "n_samples = 60000\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5gfB2irJ4Jdz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare the dataset\n",
        "`Dataset.from_tensor_slices` accepts one argument that can consist of seveal tf.tensors oder np.ndarrays. It can give out slices of the dataset (eg. batches), whereas `Dataset.from_tensors` can only give out the whole dataset. `Dataset.from_generator` lets you create the dataset at runtime via a generator function. This is useful for huge datasets that don't fit on your harddrive.\n",
        "\n",
        "`Dataset.batch(int)` is used to split the dataset into batches of size `int`. \n",
        "\n",
        "`Dataset.repeat(int)` duplicates and concatenates the duplicate with the existing dataset `int` times. Without an argument, this function just repeats the dataset as often as needed for the evaluation of a graph. \n",
        "\n",
        "`Dataset.map(fn)` applies the function `fn` on all elements of the dataset elementwise. \n",
        "\n",
        "`Dataset.filter(cond)` lets you filter the dataset based on condition `cond`, \n",
        "\n",
        "`Dataset.zip()` allows you to zip together different Dataset objects, just like the python zip function.\n",
        "\n",
        "More on this topic: [tensorflow-dataset-tutorial](http://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/)"
      ]
    },
    {
      "metadata": {
        "id": "z7FsYAR7_ELX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set up a Dataset object for the train data\n",
        "data_train = tf.data.Dataset.from_tensor_slices((tf.cast(x_train, tf.float32), y_train))\n",
        "data_train = data_train.repeat().batch(200)\n",
        "# set up a Dataset object for the test data\n",
        "data_test = tf.data.Dataset.from_tensor_slices((tf.cast(x_test, tf.float32), y_test))\n",
        "data_test = data_test.repeat().batch(200)\n",
        "# create a general iterator for the datasets and the get_next method\n",
        "iterator = tf.data.Iterator.from_structure(data_train.output_types,\n",
        "                                           data_train.output_shapes)\n",
        "iterator_test = tf.data.Iterator.from_structure(data_test.output_types,\n",
        "                                           data_test.output_shapes)\n",
        "next_element = iterator.get_next()\n",
        "next_element_test = iterator_test.get_next()\n",
        "# create Iterator initiliazers for both Datasets\n",
        "training_init_op = iterator.make_initializer(data_train)\n",
        "test_init_op = iterator.make_initializer(data_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nk05MgOx96FD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we wrap the tensorflow `conv2d` and `max_pool` functions, the convolution kernel size is defined by the shape of $W$, the pooling kernel size has to be declared."
      ]
    },
    {
      "metadata": {
        "id": "og51FiOD96FF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def maxpool2d(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lziGQQlB96FM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "Next we build the CNN model. First we declare the variables, then the computational graph."
      ]
    },
    {
      "metadata": {
        "id": "M6MNGp9796FO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_nodes_hl1 = 32\n",
        "n_nodes_hl2 = 64\n",
        "n_nodes_hl3 = 1024\n",
        "n_classes = 10\n",
        "batch_size = 200\n",
        "\n",
        "\n",
        "def convolutional_neural_network(x):\n",
        "    weights = {\n",
        "        # 5 x 5 convolution, 1 input image, 32 outputs\n",
        "        'W_conv1': tf.Variable(tf.random_normal([5, 5, 1, n_nodes_hl1])),\n",
        "        # 5x5 conv, 32 inputs, 64 outputs \n",
        "        'W_conv2': tf.Variable(tf.random_normal([5, 5, n_nodes_hl1, n_nodes_hl2])),\n",
        "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
        "        'W_fc': tf.Variable(tf.random_normal([7*7*n_nodes_hl2, n_nodes_hl3])),\n",
        "        # 1024 inputs, 10 outputs (class prediction)\n",
        "        'out': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes]))\n",
        "    }\n",
        "\n",
        "    biases = {\n",
        "        'b_conv1': tf.Variable(tf.random_normal([n_nodes_hl1])),\n",
        "        'b_conv2': tf.Variable(tf.random_normal([n_nodes_hl2])),\n",
        "        'b_fc': tf.Variable(tf.random_normal([n_nodes_hl3])),\n",
        "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    # Reshape input to a 4D tensor \n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "    # Convolution Layer, using our function\n",
        "    x = tf.nn.relu(conv2d(x, weights['W_conv1']) + biases['b_conv1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    x = maxpool2d(x)\n",
        "    # Convolution Layer\n",
        "    x = tf.nn.relu(conv2d(x, weights['W_conv2']) + biases['b_conv2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    x = maxpool2d(x)\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer\n",
        "    x = tf.reshape(x, [-1, 7*7*n_nodes_hl2])\n",
        "    x = tf.nn.relu(tf.matmul(x, weights['W_fc']) + biases['b_fc'])\n",
        "    output = tf.matmul(x, weights['out']) + biases['out']\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G7G04o-696FV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next: write a training function:"
      ]
    },
    {
      "metadata": {
        "id": "2m2lJRCu96FX",
        "colab_type": "code",
        "outputId": "4235e2f2-9268-4348-e06d-9693f99b97c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network():\n",
        "    # declare all important functions and the model graph first\n",
        "    prediction = convolutional_neural_network(next_element[0])\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=next_element[1]))\n",
        "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "    equality = tf.equal(tf.to_float(tf.argmax(prediction,1)), tf.to_float(tf.argmax(next_element[1], 1)))    \n",
        "    accuracy = tf.reduce_mean(tf.to_float(equality))\n",
        "    hm_epochs = 10\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(training_init_op)\n",
        "        for epoch in range(hm_epochs):\n",
        "            epoch_loss = 0\n",
        "            for _ in range(int(n_samples/batch_size)):\n",
        "                _, c = sess.run([optimizer, cost])\n",
        "                epoch_loss += c\n",
        "\n",
        "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "        # initialize test data as data source and compute the accuracy\n",
        "        sess.run(test_init_op)\n",
        "        avg_acc = 0\n",
        "        for i in range(100):\n",
        "          acc = sess.run([accuracy])\n",
        "          avg_acc +=acc[0]\n",
        "        print('Accuracy:',avg_acc)\n",
        "train_neural_network()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 completed out of 10 loss: 983986.0851287842\n",
            "Epoch 1 completed out of 10 loss: 206543.25619506836\n",
            "Epoch 2 completed out of 10 loss: 126760.17093205452\n",
            "Epoch 3 completed out of 10 loss: 89780.49850058556\n",
            "Epoch 4 completed out of 10 loss: 63913.13455379009\n",
            "Epoch 5 completed out of 10 loss: 50297.55650415834\n",
            "Epoch 6 completed out of 10 loss: 38080.19561898708\n",
            "Epoch 7 completed out of 10 loss: 29794.981118291616\n",
            "Epoch 8 completed out of 10 loss: 26291.624722008706\n",
            "Epoch 9 completed out of 10 loss: 19571.69092036411\n",
            "Accuracy: 96.72000002861023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CMwXH7dz96Fc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ResNets"
      ]
    },
    {
      "metadata": {
        "id": "_fZPR17rYaGe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Residual Block\n",
        "We implement the right one\n",
        "![grafik.png](https://i.stack.imgur.com/lFNWB.png)"
      ]
    },
    {
      "metadata": {
        "id": "0TKsM7ul96Ff",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resblock(x, filters, is_training):\n",
        "    x = tf.layers.batch_normalization(x,training=is_training)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.layers.conv2d(x,filters,(3,3),padding='same')\n",
        "    x = tf.layers.batch_normalization(x,training=is_training)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.layers.conv2d(x,filters,(3,3),padding='same')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "0_15u6n296Fp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ResNet(x):\n",
        "    # declare placeholder to indicate if it's training or testing\n",
        "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
        "    filters = 64\n",
        "    # Reshape input to a 4D tensor \n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "    x = tf.layers.conv2d(x, filters=filters, kernel_size=(3,3),padding='same')\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x = maxpool2d(x)\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x = maxpool2d(x)\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x = tf.reshape(x, [-1, 7*7*filters])\n",
        "    x = tf.layers.dense(x,512,activation='relu')\n",
        "    out = tf.layers.dense(x,10,activation='softmax')\n",
        "    return out\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RmywA1olM-j0",
        "colab_type": "code",
        "outputId": "0bbfede1-a089-4bd2-b5a0-afcf0d34a6d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network():\n",
        "    # declare some variables\n",
        "    n_classes = 10\n",
        "    batch_size = 200\n",
        "    hm_epochs = 10\n",
        "    # declare all important functions and the model graph\n",
        "    prediction = ResNet(next_element[0])\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=next_element[1]))\n",
        "    equality = tf.equal(tf.to_float(tf.argmax(prediction,1)), tf.to_float(tf.argmax(next_element[1], 1)))    \n",
        "    accuracy = tf.reduce_mean(tf.to_float(equality))\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    # make sure to update_ops when running the optimizer, otherwise the population moments\n",
        "    # inside batch_norm are not calculated correctly and BN doesn't work\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
        "    with tf.Session() as sess:\n",
        "        # initialize variables and interator\n",
        "        sess.run([tf.global_variables_initializer(),training_init_op])\n",
        "        for epoch in range(hm_epochs):\n",
        "            epoch_loss = 0\n",
        "            for _ in range(int(n_samples/batch_size)):\n",
        "                _, c = sess.run([optimizer, cost],feed_dict={'is_training:0':True})\n",
        "                epoch_loss += c\n",
        "\n",
        "            print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "        sess.run(test_init_op) \n",
        "        avg_acc = 0\n",
        "        for i in range(100):\n",
        "          acc = sess.run([accuracy],feed_dict={'is_training:0':False})\n",
        "          avg_acc +=acc[0]\n",
        "        print('Accuracy:',avg_acc)\n",
        "\n",
        "train_neural_network()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed out of 10 loss: 513.4809859991074\n",
            "Epoch 2 completed out of 10 loss: 454.109428524971\n",
            "Epoch 3 completed out of 10 loss: 441.7268034219742\n",
            "Epoch 4 completed out of 10 loss: 440.61018764972687\n",
            "Epoch 5 completed out of 10 loss: 440.0227122306824\n",
            "Epoch 6 completed out of 10 loss: 439.68425619602203\n",
            "Epoch 7 completed out of 10 loss: 439.549938082695\n",
            "Epoch 8 completed out of 10 loss: 439.5607305765152\n",
            "Epoch 9 completed out of 10 loss: 439.3178821802139\n",
            "Epoch 10 completed out of 10 loss: 439.31312918663025\n",
            "Accuracy: 98.83000075817108\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}