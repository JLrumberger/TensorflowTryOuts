{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4 FishNet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JLrumberger/TensorflowTryOuts/blob/master/4_FishNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fglcuKPUKt8Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Authenticate gcs bucket"
      ]
    },
    {
      "metadata": {
        "id": "-o0KTZLb_gHJ",
        "colab_type": "code",
        "outputId": "9bbb4466-2689-4c26-a478-222eac13971c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "use_tpu = True #@param {type:\"boolean\"}\n",
        "bucket = 'gcolab' #@param {type:\"string\"}\n",
        "\n",
        "assert bucket, 'Must specify an existing GCS bucket name'\n",
        "print('Using bucket: {}'.format(bucket)) #gcolab\n",
        "\n",
        "if use_tpu:\n",
        "    assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "MODEL_DIR = 'gs://{}/{}'.format(bucket, time.strftime('tpuestimator/%Y-%m-%d-%H-%M-%S'))\n",
        "print('Using model dir: {}'.format(MODEL_DIR))\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "  \n",
        "  # Upload credentials to TPU.\n",
        "  with tf.Session(TF_MASTER) as sess:    \n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n",
        "else:\n",
        "  TF_MASTER=''\n",
        "\n",
        "with tf.Session(TF_MASTER) as session:\n",
        "  pprint.pprint(session.list_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using bucket: gcolab\n",
            "Using model dir: gs://gcolab/tpuestimator/2019-01-24-13-01-07\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 15154662220332540682),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14132379737731697547),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 8747735079900212106),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2318908220440493821),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 77492806112203606),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4001567152007306733),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 100740209273723394),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14292765378049059121),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14735982531692235965),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 9822894702633967358),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7573220763219786352),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9427893794185109654)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DV0LBEYuKzY9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load data and set parameters"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "6bbff93b-7d10-4b1a-8a3f-32f157ee314b",
        "id": "OsEMXCPjKlpM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "# Load training and eval datasets\n",
        "import numpy as np\n",
        "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
        "train_data = mnist.train.images # Returns np.array\n",
        "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
        "eval_data = mnist.test.images # Returns np.array\n",
        "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
        "\n",
        "params = {\"x_train\": {\"x\": train_data},\n",
        "          \"y_train\": train_labels,\n",
        "          \"x_test\" : {\"x\": eval_data},\n",
        "          \"y_test\" : eval_labels,\n",
        "          \"iterations_per_loop\": 1024, # after each loop the TPU passes back information to host CPU\n",
        "          \"train_batch_size\":1024,\n",
        "          \"eval_batch_size\":1024,\n",
        "          \"predict_batch_size\":1024,\n",
        "          \"epochs\": 20}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GP2wzB_gYCHV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  FishNet Backbone\n",
        "A little [FishNet](https://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf) implementation\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/kevin-ssy/FishNet/master/head_pic.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "QTawbsuQYJlc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Upsampling & Refinement Block\n",
        ".. and the rest of the blocks used"
      ]
    },
    {
      "metadata": {
        "id": "EiAuYqEaYINm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bottleneck_Resblock(x, filters, is_training,dilation):\n",
        "    x = tf.layers.batch_normalization(x,training=is_training)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.layers.conv2d(x,filters,(1,1),padding='same')    \n",
        "    x = tf.layers.batch_normalization(x,training=is_training)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.layers.conv2d(x,filters,(3,3),padding='same',dilation_rate=dilation)    \n",
        "    x = tf.layers.batch_normalization(x,training=is_training)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.layers.conv2d(x,filters,(1,1),padding='same')    \n",
        "    return x\n",
        "  \n",
        "def resblock(x, filters, is_training):\n",
        "    x = tf.layers.batch_normalization(x,training=is_training)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.layers.conv2d(x,filters,(3,3),padding='same')\n",
        "    x = tf.layers.batch_normalization(x,training=is_training)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = tf.layers.conv2d(x,filters,(3,3),padding='same')\n",
        "    return x\n",
        "  \n",
        "def fishblock(a_prev,a_trans, is_training, k, mode):\n",
        "    # a_prev contains the feature maps from the previous layer\n",
        "    # a_trans contains the feature maps transfered from the fishtail\n",
        "    x_sh = tf.concat([a_prev,a_trans], axis=3)\n",
        "    n,h,w,c = x_sh.get_shape().as_list()\n",
        "    m = bottleneck_Resblock(x_sh, c//k, is_training,(1, 1))\n",
        "    if mode == \"UP\":\n",
        "        r = tf.reduce_sum(tf.reshape(x_sh,[n, h, w, c//k, k]), axis=4,keepdims=False)\n",
        "        out = tf.layers.conv2d_transpose(m+r,c//k,(2,2),(2,2))\n",
        "    if mode == \"DOWN\":\n",
        "        out = tf.nn.max_pool(x_sh+m, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "    return out\n",
        "  \n",
        "def se_block(x, out_ch, is_training):\n",
        "    n,h,w,c = x.get_shape()\n",
        "    squeeze = tf.layers.average_pooling2d(x,[h,w],[h,w],\"valid\")\n",
        "    excitation = tf.layers.conv2d(squeeze,out_ch//16,(1,1),padding='same')\n",
        "    excitation = tf.nn.relu(excitation)\n",
        "    excitation = tf.layers.conv2d(excitation,out_ch,(1,1),padding='same')\n",
        "    excitation = tf.nn.sigmoid(excitation)\n",
        "    excitation = tf.reshape(excitation, [-1,1,1,out_ch])\n",
        "    scale = x * excitation\n",
        "    return scale\n",
        "  \n",
        "def maxpool2d(x):\n",
        "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "  \n",
        "def Fishnet(x,is_training):\n",
        "    \"\"\" original 224->56->28->14->7->1->7->14->28->56->28->14->7->1\n",
        "        ours 28->14->7->1->7->14->28->14->7->1 for mnist\n",
        "    \"\"\"\n",
        "    filters = 32\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "    # TAIL\n",
        "    x = tf.layers.conv2d(x, filters=filters, kernel_size=(3,3),padding='same')\n",
        "    hw_28 = x\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x = maxpool2d(x) # pools to 14x14\n",
        "    hw_14 = x\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x = maxpool2d(x) # pools to 7x7\n",
        "    hw_7 = x\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x += se_block(x,32,is_training) \n",
        "    # BODY\n",
        "    x = fishblock(x,hw_7,is_training,2,\"UP\") # resizes to 14x14\n",
        "    hw_14_2 = x\n",
        "    x += resblock(x,filters,is_training)\n",
        "    x = fishblock(x,hw_14,is_training,2,\"UP\") # resizes to 28x28\n",
        "    hw_28_2 = x\n",
        "    x += resblock(hw_28_2,filters,is_training)\n",
        "    # HEAD\n",
        "    x = fishblock(x,hw_28_2,is_training,1,\"DOWN\") # resizes to 14x14, outputs c(32,32) = 64 channels\n",
        "    filters = 96\n",
        "    x = fishblock(x,hw_14_2,is_training,1,\"DOWN\") # resizes to 7x7, outputs c(64,32) = 96 channels\n",
        "    x = tf.reshape(x, [-1, 7*7*filters])\n",
        "    x = tf.layers.dense(x,512,activation='relu')\n",
        "    out = tf.layers.dense(x,10)\n",
        "    return out\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ldU8J8jD8zKP",
        "colab_type": "code",
        "outputId": "8e12453f-c258-42c6-f429-ea33facd5af0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3527
        }
      },
      "cell_type": "code",
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "  # Specify the model\n",
        "  logits = Fishnet(x = features[\"x\"],\n",
        "                  is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n",
        "\n",
        "  predictions = {\n",
        "      # Generate predictions (for PREDICT and EVAL mode)\n",
        "      \"classes\": tf.argmax(input=logits, axis=1),\n",
        "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
        "      # `logging_hook`.\n",
        "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
        "  }\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, predictions=predictions)\n",
        "\n",
        "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "  # Configure the Training Op (for TRAIN mode)\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    learning_rate = tf.train.exponential_decay(0.001,\n",
        "          #FLAGS.learning_rate,\n",
        "          tf.train.get_global_step(),\n",
        "          decay_steps=100000,\n",
        "          decay_rate=0.96)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "    # update op for batch_norm layer\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        train_op = optimizer.minimize(loss=loss,\n",
        "            global_step=tf.train.get_global_step())\n",
        "    return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "  # Add evaluation metrics (for EVAL mode)\n",
        "  return tf.contrib.tpu.TPUEstimatorSpec(\n",
        "      mode=mode, loss=loss, eval_metrics=(metric_fn,[labels,logits]))\n",
        "\n",
        "  \n",
        "def metric_fn(labels, logits):\n",
        "    accuracy = tf.metrics.accuracy(\n",
        "        labels=labels, predictions=tf.argmax(logits, axis=1))\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Create model input functions\n",
        "def train_input_fn(params):\n",
        "  # Convert the inputs to a Dataset.\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((params[\"x_train\"], params[\"y_train\"]))\n",
        "  # Shuffle, repeat, and batch the examples.\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "  dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True)\n",
        "  return dataset\n",
        "\n",
        "def eval_input_fn(params):\n",
        "  # Convert the inputs to a Dataset.\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((params[\"x_test\"], params[\"y_test\"]))\n",
        "  # Shuffle, repeat, and batch the examples.\n",
        "  dataset = dataset.shuffle(1000).repeat()\n",
        "  dataset = dataset.batch(params[\"batch_size\"], drop_remainder=True)\n",
        "  return dataset\n",
        "\n",
        "def predict_input_fn(params):\n",
        "  # generates MC dropout probabilities\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(params[\"x_test\"])\n",
        "  dataset = dataset.repeat(params[\"nb_MC_samples\"])\n",
        "  dataset = dataset.batch(params[\"batch_size\"])\n",
        "  return dataset\n",
        "  \n",
        "# Create the Estimator\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=MODEL_DIR, # google cloud services bucket\n",
        "    session_config=tf.ConfigProto(\n",
        "        allow_soft_placement=True, log_device_placement=True),\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(params[\"iterations_per_loop\"])\n",
        ")\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    model_fn=model_fn,\n",
        "    use_tpu=True,\n",
        "    train_batch_size=params[\"train_batch_size\"],\n",
        "    eval_batch_size=params[\"eval_batch_size\"],\n",
        "    predict_batch_size=params[\"predict_batch_size\"],\n",
        "    config=run_config,\n",
        "    params=params\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "\n",
        "estimator.train(input_fn=train_input_fn, steps=params[\"epochs\"]*params[\"train_batch_size\"])\n",
        "\n",
        "estimator.evaluate(input_fn= eval_input_fn, steps=params[\"eval_steps\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://gcolab/tpuestimator/2019-01-24-13-01-07', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "log_device_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      value: \"10.80.133.34:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f559109f358>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': b'grpc://10.80.133.34:8470', '_evaluation_master': b'grpc://10.80.133.34:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1024, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None), '_cluster': <tensorflow.contrib.cluster_resolver.python.training.tpu_cluster_resolver.TPUClusterResolver object at 0x7f558bb5c4e0>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Querying Tensorflow master (b'grpc://10.80.133.34:8470') for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 15154662220332540682)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14132379737731697547)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 8747735079900212106)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2318908220440493821)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 77492806112203606)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 4001567152007306733)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 100740209273723394)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14292765378049059121)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 14735982531692235965)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 9822894702633967358)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 7573220763219786352)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 9427893794185109654)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into gs://gcolab/tpuestimator/2019-01-24-13-01-07/model.ckpt.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:tpu_worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.10217657, step = 1024\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.09380906, step = 2048 (18.999 sec)\n",
            "INFO:tensorflow:global_step/sec: 53.8961\n",
            "INFO:tensorflow:examples/sec: 55189.6\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.081646055, step = 3072 (23.995 sec)\n",
            "INFO:tensorflow:global_step/sec: 42.675\n",
            "INFO:tensorflow:examples/sec: 43699.2\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.0446425, step = 4096 (17.287 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2353\n",
            "INFO:tensorflow:examples/sec: 60657\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.0464426, step = 5120 (17.285 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2414\n",
            "INFO:tensorflow:examples/sec: 60663.2\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.022340639, step = 6144 (17.291 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.218\n",
            "INFO:tensorflow:examples/sec: 60639.2\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.030514793, step = 7168 (17.282 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2536\n",
            "INFO:tensorflow:examples/sec: 60675.6\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.030713096, step = 8192 (17.283 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2461\n",
            "INFO:tensorflow:examples/sec: 60668\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.015842196, step = 9216 (17.274 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.283\n",
            "INFO:tensorflow:examples/sec: 60705.8\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.021381723, step = 10240 (23.889 sec)\n",
            "INFO:tensorflow:global_step/sec: 42.865\n",
            "INFO:tensorflow:examples/sec: 43893.8\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.024069404, step = 11264 (17.285 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2443\n",
            "INFO:tensorflow:examples/sec: 60666.2\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.008201163, step = 12288 (17.267 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.302\n",
            "INFO:tensorflow:examples/sec: 60725.3\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.026818175, step = 13312 (17.286 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2411\n",
            "INFO:tensorflow:examples/sec: 60662.9\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.016521636, step = 14336 (17.289 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2221\n",
            "INFO:tensorflow:examples/sec: 60643.4\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.00506762, step = 15360 (17.278 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2687\n",
            "INFO:tensorflow:examples/sec: 60691.1\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.022370892, step = 16384 (24.743 sec)\n",
            "INFO:tensorflow:global_step/sec: 41.3857\n",
            "INFO:tensorflow:examples/sec: 42379\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.011444247, step = 17408 (17.289 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2277\n",
            "INFO:tensorflow:examples/sec: 60649.1\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.011889105, step = 18432 (17.282 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2539\n",
            "INFO:tensorflow:examples/sec: 60676\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.0128935985, step = 19456 (17.274 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2798\n",
            "INFO:tensorflow:examples/sec: 60702.5\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.015794482, step = 20480 (17.280 sec)\n",
            "INFO:tensorflow:global_step/sec: 59.2543\n",
            "INFO:tensorflow:examples/sec: 60676.4\n",
            "INFO:tensorflow:Saving checkpoints for 20480 into gs://gcolab/tpuestimator/2019-01-24-13-01-07/model.ckpt.\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Loss for final step: 0.015794482.\n",
            "INFO:tensorflow:training_loop marked as finished\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2019-01-24-13:10:04\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://gcolab/tpuestimator/2019-01-24-13-01-07/model.ckpt-20480\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Init TPU system\n",
            "INFO:tensorflow:Initialized TPU in 7 seconds\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Enqueue next (1024) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1024) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Evaluation [1024/1024]\n",
            "INFO:tensorflow:Stop infeed thread controller\n",
            "INFO:tensorflow:Shutting down InfeedController thread.\n",
            "INFO:tensorflow:InfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Infeed thread finished, shutting down.\n",
            "INFO:tensorflow:infeed marked as finished\n",
            "INFO:tensorflow:Stop output thread controller\n",
            "INFO:tensorflow:Shutting down OutfeedController thread.\n",
            "INFO:tensorflow:OutfeedController received shutdown signal, stopping.\n",
            "INFO:tensorflow:Outfeed thread finished, shutting down.\n",
            "INFO:tensorflow:outfeed marked as finished\n",
            "INFO:tensorflow:Shutdown TPU system.\n",
            "INFO:tensorflow:Finished evaluation at 2019-01-24-13:10:21\n",
            "INFO:tensorflow:Saving dict for global step 20480: accuracy = 0.9876995, global_step = 20480, loss = 0.036881298\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 20480: gs://gcolab/tpuestimator/2019-01-24-13-01-07/model.ckpt-20480\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.9876995, 'global_step': 20480, 'loss': 0.036881298}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}